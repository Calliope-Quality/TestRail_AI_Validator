{
 "cells": [
  {
   "cell_type": "code",
   "id": "f3ad3691d9c8ec29",
   "metadata": {},
   "source": [
    "import collections\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "import pprint\n",
    "import datetime\n",
    "import csv\n",
    "import collections\n",
    "import concurrent.futures\n",
    "import openai\n",
    "\n",
    "env_path = Path(\"..\") / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "TESTRAIL_BASE_URL = os.getenv(\"TESTRAIL_BASE_URL\")\n",
    "TESTRAIL_USERNAME = os.getenv(\"TESTRAIL_USERNAME\")\n",
    "TESTRAIL_API_KEY = os.getenv(\"TESTRAIL_API_KEY\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL='gpt-4o-mini'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### log debug to file",
   "id": "1b3c3ff2b25f2711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "now = datetime.datetime.now(datetime.UTC)\n",
    "formated_date = now.strftime(\"%Y%m%d\")\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Log level\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',  # Log format\n",
    "    filename='logs/{formated_date}-app.log',  # Log file name\n",
    "    filemode='a'  # 'a' for append mode, 'w' to overwrite\n",
    ")"
   ],
   "id": "e9444a91ee8c0ee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  helper to convert datestrings",
   "id": "2dfc37b079bbf64f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_unix_time(date_str: str=\"01/01/2023\"):\n",
    "    \"\"\"\n",
    "    convert a mm/dd/yyyy format to unix time\n",
    "    Args:\n",
    "        date_str: mm/dd/yyyy format\n",
    "\n",
    "    Returns: seconds since epoch\n",
    "    \"\"\"\n",
    "    date_obj = datetime.datetime.strptime(date_str, \"%m/%d/%Y\")\n",
    "    return int(time.mktime(date_obj.timetuple()))\n",
    "\n",
    "def get_human_time(seconds: str) -> str:\n",
    "    \"\"\"\n",
    "    takes in a string of unix seconds (generally an int, but we are converting here)\n",
    "    Args:\n",
    "        seconds:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    date = datetime.datetime.fromtimestamp(seconds)\n",
    "    return date.strftime(\"%m/%d/%Y\")"
   ],
   "id": "6b4cc80ec7acd410",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### updated functions, a helper or two maybe",
   "id": "4749b78129bf5861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate_env() -> bool:\n",
    "    \"\"\"\n",
    "    validate environment variables\n",
    "    Returns: boolean\n",
    "    \"\"\"\n",
    "    if not TESTRAIL_BASE_URL or not TESTRAIL_USERNAME or not TESTRAIL_API_KEY:\n",
    "        print(\"Error: One or more environment variables are missing. Check your .env file.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ],
   "id": "77e59ee7404a2c05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_projects(limit=250) -> list:\n",
    "    \"\"\"\n",
    "    fetch testrails projects\n",
    "    Returns: json/dict of projects\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the API URL\n",
    "    try:\n",
    "        # Make the API request\n",
    "        url = f\"{TESTRAIL_BASE_URL}/index.php?/api/v2/get_projects&limit={limit}\"\n",
    "        response = requests.get(url, auth=(TESTRAIL_USERNAME, TESTRAIL_API_KEY))\n",
    "        return response.json()['projects']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to TestRail API: {e}\")\n"
   ],
   "id": "e7bc7453edc9b719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_suites(project_id) -> list:\n",
    "    \"\"\"\n",
    "    fetch testrails suites\n",
    "    Returns: list of suites\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make the API request\n",
    "        url = f\"{TESTRAIL_BASE_URL}/index.php?/api/v2/get_suites/{project_id}\"\n",
    "        response = requests.get(url, auth=(TESTRAIL_USERNAME, TESTRAIL_API_KEY))\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to TestRail API: {e}\")"
   ],
   "id": "a76ddcc3c94f4990",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fetch_test_cases(project_id, suite_id, date=\"01/01/2022\", limit=250, wait=0) -> list:\n",
    "    \"\"\"\n",
    "    fetch testrails test cases\n",
    "    Args:\n",
    "        project_id:\n",
    "        suite_id:\n",
    "        date: optional, mm/dd/yyyy format. default 01/01/2022\n",
    "        limit: optional, default 250, set to whatever number we want to pull at once\n",
    "        wait: optional, number of seconds to wait between requests\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not TESTRAIL_USERNAME or not TESTRAIL_API_KEY or not TESTRAIL_API_KEY:\n",
    "        raise Exception(\"Error: Missing TestRail configuration in environment variables.\")\n",
    "\n",
    "    date_filter = \"created_after\" # alternatey we could try \"updated_after\"\n",
    "    url = f\"{TESTRAIL_BASE_URL}/index.php?/api/v2/get_cases/{project_id}&suite_id={suite_id}&{date_filter}={get_unix_time(date)}&limit={limit}\"\n",
    "    output = []\n",
    "    while url is not None:\n",
    "        print(url)\n",
    "        try:\n",
    "            response = requests.get(url, auth=(TESTRAIL_USERNAME, TESTRAIL_API_KEY))\n",
    "            payload = response.json()\n",
    "            output.extend(payload['cases'])\n",
    "            next = payload['_links']['next']\n",
    "            if next:\n",
    "                url = f\"{TESTRAIL_BASE_URL}/index.php?{next}\"\n",
    "            else :\n",
    "                url = None\n",
    "            if wait > 0:\n",
    "                time.sleep(wait)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    return output\n"
   ],
   "id": "7e3579165c00c2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cases = []\n",
    "if validate_env():\n",
    "    projects = fetch_projects()\n",
    "    project_ids = [project['id'] for project in projects]\n",
    "    for pid in project_ids:\n",
    "        suite_ids = [_['id'] for _ in fetch_suites(pid)]\n",
    "        for sid in suite_ids:\n",
    "            c = fetch_test_cases(pid, sid, date=\"06/01/2024\")\n",
    "            for case in c:\n",
    "                case['project_id'] = pid\n",
    "                case['suite_id'] = sid\n",
    "            cases.extend(c)"
   ],
   "id": "74ea634535cd4609",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## we now have all our test cases in a list.",
   "id": "e1e42575ecc37b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pprint.pprint(cases[0])  # just to see what the output looks like\n",
    "print()\n",
    "print(get_human_time(cases[0]['created_on']))\n",
    "print(get_human_time(cases[0]['updated_on']))\n"
   ],
   "id": "6d614145e9c3cb43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(cases))",
   "id": "622ff18c14f45ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is just the same thing from evaluate with the model as an optional argument\n",
    "def evaluate_test_case(test_case):\n",
    "    \"\"\"\n",
    "    Sends a test case to OpenAI for evaluation based on QA best practices.\n",
    "\n",
    "    Parameters:\n",
    "        test_case (dict): A dictionary containing the test case details.\n",
    "\n",
    "    Returns:\n",
    "        str: OpenAI's evaluation of the test case, including a grade and recommendations.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the OpenAI API request fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # Safely extract fields with fallbacks\n",
    "    description = test_case.get('description') or test_case.get('custom_testcase_description', 'N/A')\n",
    "    preconditions = test_case.get('preconditions') or test_case.get('custom_preconds', 'N/A')\n",
    "\n",
    "    # Handle 'steps'\n",
    "    steps_separated = test_case.get('custom_steps_separated', [])\n",
    "    if isinstance(steps_separated, list):\n",
    "        steps = \"\\n\".join([step.get('content', '') for step in steps_separated])\n",
    "    else:\n",
    "        steps = test_case.get('custom_steps', 'N/A')\n",
    "\n",
    "    expected_result = test_case.get('expected_result') or test_case.get('custom_expected', 'N/A')\n",
    "    created_by = test_case.get('created_by', 'Unknown Creator')\n",
    "\n",
    "    # Build the prompt from the test case details\n",
    "    prompt = (\n",
    "        f\"Evaluate the following test case based on QA best practices.\"\n",
    "        f\"Grade the test case quality with one of the following categories: \"\n",
    "        f\"'No improvement needed,' 'Needs slight improvement,' or 'Needs major improvement.' \"\n",
    "        f\"Additionally, review the test case for adherence to US mental healthcare standards and best practices, \"\n",
    "        f\"and identify any gaps or improvements that would ensure the system meets compliance, user needs, and clinical effectiveness:\\n\\n\"\n",
    "    f\"ID: {test_case.get('id', 'N/A')}\\n\"\n",
    "    f\"Title: {test_case.get('title', 'N/A')}\\n\"\n",
    "    f\"Description: {description}\\n\"\n",
    "    f\"Preconditions: {preconditions}\\n\"\n",
    "    f\"Steps: {steps}\\n\"\n",
    "    f\"Expected Result: {expected_result}\\n\"\n",
    "    f\"Created By: {created_by}\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create a chat completion\n",
    "        completion = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\",\"content\": \"You are an expert in software testing, quality assurance, and electronic medical records (EMRs), especially in mental healthcare systems. Your goal is to evaluate test cases not only for QA best practices but also for their adherence to mental healthcare standards, compliance requirements, and clinical usability in an EMR system.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        # Assuming 'response' is the variable holding the API response\n",
    "        message_content = completion.choices[0].message.content\n",
    "        return(message_content)\n",
    "        # return(completion)\n",
    "\n",
    "    except openai.APIError as e:\n",
    "        # Handle API errors (e.g., invalid requests or server errors)\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        return f\"Error: API Error - {e}\"\n",
    "    except openai.APIConnectionError as e:\n",
    "        # Handle connection errors (e.g., network issues)\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "        return f\"Error: Connection Error - {e}\"\n",
    "    except openai.RateLimitError as e:\n",
    "        # Handle rate limit errors (e.g., too many requests)\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "        return f\"Error: Rate Limit Exceeded - {e}\"\n",
    "    except Exception as e:\n",
    "        # Handle unexpected exceptions\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return f\"Error: Unexpected Error - {e}\""
   ],
   "id": "3d499f3ec65d261f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_case(case_with_index):\n",
    "    \"\"\"\n",
    "    Processes a single case and returns its evaluation with project/suite information\n",
    "    \"\"\"\n",
    "    idx, total, case = case_with_index\n",
    "    print(f\"{idx}/{total} - {case['project_id']}:{case['suite_id']}:{case['id']}: {case['title']}\")\n",
    "\n",
    "    eval_result = evaluate_test_case(case)\n",
    "\n",
    "    # Extract steps and expected results exactly as in your sequential code\n",
    "    steps = []\n",
    "    expected_results = []\n",
    "    if case['custom_steps_separated'] is not None:\n",
    "        steps = [step['content'] for step in case['custom_steps_separated'] if 'content' in step.keys()]\n",
    "        expected_results = [step['expected'] for step in case['custom_steps_separated'] if 'expected' in step.keys()]\n",
    "\n",
    "    if case['title'] is None:\n",
    "        case['title'] = f\"BAD TESTCASE: No Title\"\n",
    "\n",
    "    if case['custom_testcase_description'] is None:\n",
    "        case['custom_testcase_description'] = f\"BAD TESTCASE: No Description\"\n",
    "\n",
    "    if case['custom_preconds'] is None:\n",
    "        case['custom_preconds'] = f\"BAD TESTCASE: No Preconditions\"\n",
    "\n",
    "    return {\n",
    "        'project_id': case['project_id'],\n",
    "        'suite_id': case['suite_id'],\n",
    "        'evaluation_data': {\n",
    "            'id': case['id'],\n",
    "            'title': case['title'],\n",
    "            'description': case['custom_testcase_description'],\n",
    "            'steps': steps,\n",
    "            'expected_result': expected_results,\n",
    "            'preconditions': case['custom_preconds'],\n",
    "            'created_by': case['created_by'],\n",
    "            'created_on': get_human_time(case['created_on']),\n",
    "            'evaluation': eval_result\n",
    "        }\n",
    "    }\n",
    "\n",
    "def parallel_evaluate_cases(cases, max_workers=10):\n",
    "    \"\"\"\n",
    "    Evaluates test cases in parallel while maintaining the same output structure\n",
    "    \"\"\"\n",
    "    evaluations = collections.defaultdict(dict)\n",
    "    total = len(cases)\n",
    "\n",
    "    # Prepare cases with their indices for progress tracking\n",
    "    cases_with_index = [(i+1, total, case) for i, case in enumerate(cases)]\n",
    "\n",
    "    # Process cases in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Using list() to force immediate execution and show progress\n",
    "        results = list(executor.map(process_case, cases_with_index))\n",
    "\n",
    "    # Organize results into the same structure as your sequential code\n",
    "    for result in results:\n",
    "        pid = result['project_id']\n",
    "        sid = result['suite_id']\n",
    "\n",
    "        if sid not in evaluations[pid]:\n",
    "            evaluations[pid][sid] = []\n",
    "\n",
    "        evaluations[pid][sid].append(result['evaluation_data'])\n",
    "\n",
    "    return dict(evaluations)"
   ],
   "id": "fea539e0e071905d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluations = parallel_evaluate_cases(cases, 100)",
   "id": "46d7de9c8095a7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import concurrent.futures\n",
    "\n",
    "evaluations = collections.defaultdict(dict)\n",
    "count = 0\n",
    "for c in cases:\n",
    "    count += 1\n",
    "    print(f\"{count}/{len(cases)} - {c['project_id']}:{c['suite_id']}:{c['id']}: {c['title']}\")\n",
    "    eval = evaluate_test_case(c)\n",
    "\n",
    "    steps = []\n",
    "    expected_results = []\n",
    "    if c['custom_steps_separated'] is not None:\n",
    "        steps = [step['content'] for step in c['custom_steps_separated']]\n",
    "        expected_results = [step['expected'] for step in  c['custom_steps_separated']]\n",
    "\n",
    "\n",
    "    pid = c['project_id']\n",
    "    sid = c['suite_id']\n",
    "    tid = c['id']\n",
    "\n",
    "    if pid not in evaluations.keys():\n",
    "        evaluations[pid] = {}\n",
    "    if sid not in evaluations[pid].keys():\n",
    "        evaluations[pid][sid] = []\n",
    "\n",
    "    evaluations[pid][sid].append({\n",
    "        'id' : tid,\n",
    "        'title': c['title'],\n",
    "        'description': c['custom_testcase_description'],\n",
    "        'steps': steps,\n",
    "        'expected_result': expected_results,\n",
    "        'preconditions': c['custom_preconds'],\n",
    "        'created_by': c['created_by'],\n",
    "        'created_on': get_human_time(c['created_on']),\n",
    "        'evaluation': eval\n",
    "    })"
   ],
   "id": "8dd99c8eb7607796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "count = 0\n",
    "for k, v in evaluations.items():\n",
    "    for kk, vv in v.items():\n",
    "        print(f\"project={k} suite={kk} count of cases = {len(vv)}\")\n",
    "        count += len(vv)\n",
    "print(count)"
   ],
   "id": "c3b05d08a24709cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for pid, suites in evaluations.items():\n",
    "    for sid, tests in suites.items():\n",
    "        now = datetime.datetime.now(datetime.UTC)\n",
    "        formated_date = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        filename = f\"csv_output/{formated_date}-project_{pid}-suite_{sid}-evaluations.csv\"\n",
    "        fieldnames = tests[0].keys()\n",
    "        try:\n",
    "            # with open(filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            with open(filename, mode='w', newline='', encoding='utf-16') as csv_file:\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(tests)\n",
    "            print(f\"CSV file '{filename}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ],
   "id": "67f3b70f1f20e728",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
